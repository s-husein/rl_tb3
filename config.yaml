algo_params:
  name: 'dummy_ppo'
  k_epochs: 7
  batch_size: 64
  min_batch_size: 2048
  episodes: 10000
  lam: 0.95
  gamma: 0.99
  actor_lr: 0.0005
  critic_lr: 0.001
  std_min_clip:  0.1
  eps_clip: 0.2
  beta: 0.07
  max_steps: 15000
  max_reward: 100
  
network_params:
  conv_layers: [[32, 5, 2],
                [64, 3, 1],
                [128, 3, 1]]
  max_pool: [2, 2]
  hid_layers: [512, 256]
  net_is_shared: false
  act_fn: 'relu'
  action_space: 'cont'
  init_logstd: 0.1

env_params:
  obs_scale_factor: 0.25
  positions: [[1.0, 1.0], [3, -3], [3, 3]]
  angles: [180]
  noise_std: 15
