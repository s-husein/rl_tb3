algo_params:
  name: 'dummy_ppo'
  k_epochs: 7
  batch_size: 64
  min_batch_size: 2048
  episodes: 10000
  lam: 0.95
  gamma: 0.99
  actor_lr: 0.00005
  critic_lr: 0.0001
  std_min_clip:  0.1
  eps_clip: 0.2
  beta: 0.07
  max_steps: 3000
  max_reward: 100
  
network_params:
  hid_layers: [128, 128]
  net_is_shared: false
  act_fn: 'relu'
  action_space: 'cont'
  init_logstd: 0.1