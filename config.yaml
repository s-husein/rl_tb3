algo_params:
  name: 'dummy_ppo'
  k_epochs: 10
  batch_size: 128
  min_batch_size: 2048
  episodes: 10000
  lam: 0.95
  gamma: 0.99
  actor_lr: 0.003
  critic_lr: 0.007
  std_min_clip:  0.1
  eps_clip: 0.3
  beta: 0.05
  max_steps: 2000
  max_reward: 100
  
network_params:
  hid_layers: [128, 128]
  net_is_shared: false
  act_fn: 'relu'
  action_space: 'cont'
  init_logstd: 0.5